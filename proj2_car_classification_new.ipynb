{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utcPLOa0Ot8r"
   },
   "source": [
    "# 1.introduction\n",
    "\n",
    "This project is about car classification for [stanford car dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). The Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe.\n",
    "\n",
    "It is difficult to directly train deep learning model on this dataset because the limited number of images. Thus we decide to use transfer learning, a common approch used in deep learning to utilize the pretrained model on [imagenet](http://www.image-net.org/) and fine-tune on our own dataset, i.e. car dataset.\n",
    "\n",
    "This project can show you how to train and fine-tune a deep learning model using kera (tensorflow backend).\n",
    "\n",
    "![](https://ai.stanford.edu/~jkrause/cars/class_montage.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXw-9h8vVtM-"
   },
   "source": [
    "## 1.1 Transfer Learning\n",
    "Transfer learning is one of the most widely used technologies in deep learning and computer vision. If you are not familar with the concept of transfer learning, please refer to our course materials.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*9GTEzcO8KxxrfutmtsPs3Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQX6MYbnQswu"
   },
   "source": [
    "# 2.Import & read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh72b__R9Cd"
   },
   "source": [
    "# 3.Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrbjT9XNSHaF"
   },
   "source": [
    "## 3.1 Load packages\n",
    "\n",
    "The keras and tensorflow have been pre-installled on colab, and we do not need re-install these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ll778O7XWLN0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Troy\\Anaconda3\\envs\\tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "from keras.applications import vgg19\n",
    "from keras.applications import resnet50\n",
    "from keras.applications import inception_v3\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "# import argparse\n",
    "from time import time\n",
    "\n",
    "from skimage import exposure, color\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BVSYj_zSuKP"
   },
   "source": [
    "## 3.2 Initalize model\n",
    "In this step, we will initalize our architecture. We use predefined architectures such as resnet50/vgg19/inception, you can also use your own architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNb0uGI7uHxF"
   },
   "outputs": [],
   "source": [
    "def init_model(train_dir, val_dir, batch_size=32, model_name='resnet50', num_class=196, img_size=224):\n",
    "    \"\"\"\n",
    "    initialize cnn model and training and validation data generator\n",
    "    parms:\n",
    "        args: parsed commandline arguments\n",
    "    return:\n",
    "        model: initialized model\n",
    "        train_generator: training data generator\n",
    "        validation_generator: validation data generator\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    print('loading the model and the pre-trained weights...')\n",
    "\n",
    "    # load base model\n",
    "    if model_name == 'vgg19':\n",
    "        base_model = vgg19.VGG19(include_top=False, weights='imagenet', input_shape = (img_size, img_size, 3)) # need specify input_shape\n",
    "        # this preprocess_input is the default preprocess func for given network, you can change it or implement your own \n",
    "        # use inception_v3 preprocess for vgg16, it seems that it works better than vgg16.preprocess_input\n",
    "        preprocess_input = vgg19.preprocess_input\n",
    "\n",
    "    if model_name == 'resnet50':\n",
    "        base_model = resnet50.ResNet50(weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "        preprocess_input = resnet50.preprocess_input\n",
    "    # initalize training image data generator\n",
    "    # you can also specify data augmentation here\n",
    "    train_datagen = image.ImageDataGenerator(\n",
    "        # width_shift_range=0.1,\n",
    "        # height_shift_range=0.1,\n",
    "        # samplewise_center=True,\n",
    "        # samplewise_std_normalization=True,\n",
    "        # rescale=1./255,\n",
    "        preprocessing_function=preprocess_input,\n",
    "        # rotation_range=30,\n",
    "        # shear_range=0.1,\n",
    "        # zoom_range=0.1,\n",
    "        # vertical_flip=True,\n",
    "        horizontal_flip=True\n",
    "        )\n",
    "\n",
    "    # initalize validation image data generator\n",
    "    # you can also specify data augmentation here\n",
    "    validation_datagen = image.ImageDataGenerator(\n",
    "        # samplewise_center=True,\n",
    "        # samplewise_std_normalization=True\n",
    "        # rescale=1./255\n",
    "        preprocessing_function=preprocess_input # preprocess_input\n",
    "        )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')     \n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        # color_mode='grayscale',  # 'rgb'\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # fix base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # added some customized layers for your own data\n",
    "    x = base_model.output\n",
    "    if model_name == 'vgg19':\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        # x = Flatten(name='flatten')(x)\n",
    "        # x = Dense(512, activation='relu', name='fc1-pretrain')(x)\n",
    "        x = Dense(256, activation='relu', name='fc2-pretrain')(x)\n",
    "        x = Dropout(0.3, name='dropout')(x)\n",
    "    if model_name == 'resnet50':\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        # x = Flatten(name='flatten')(x)\n",
    "        # x = Dense(512, activation='relu', name='fc1-pretrain')(x)\n",
    "        x = Dense(256, activation='relu', name='fc2-pretrain')(x)\n",
    "        x = Dropout(0.8, name='dropout')(x)\n",
    "\n",
    "    # added softmax layer\n",
    "    predictions = Dense(num_class, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam = optimizers.Adam()\n",
    "    # model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model, train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRiaN8q5TLNb"
   },
   "source": [
    "## 3.3 train model\n",
    "This part defines the function to train our model. We can specify the training parameters such as number of epochs, optimizers, and etc here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_r-lCXKuXbB"
   },
   "outputs": [],
   "source": [
    "def train(model, train_generator, validation_generator, num_class=196, model_name='resnet50', batch_size=32, epochs=30, suffix='laioffer'):\n",
    "    \"\"\"\n",
    "    train the model\n",
    "    parms:\n",
    "        model: initialized model\n",
    "        train_generator: training data generator\n",
    "        validation_generator: validation data generator\n",
    "        args: parsed command line arguments\n",
    "    return:\n",
    "    \"\"\"\n",
    "    # define number of steps/iterators per epoch\n",
    "    stepsPerEpoch = train_generator.samples / batch_size\n",
    "    validationSteps= validation_generator.samples / batch_size\n",
    "\n",
    "    # save the snapshot of the model to local drive\n",
    "    pretrain_model_name = 'pretrained_{}_{}_{}_{}.h5'.format(model_name, num_class, epochs, suffix)\n",
    "    # visualize the training process\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}_pretrain_{}\".format(model_name, time()), histogram_freq=0, write_graph=True)\n",
    "    checkpoint = ModelCheckpoint(pretrain_model_name, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    earlystopping = EarlyStopping(monitor='acc', patience=5)\n",
    "    callbacks_list = [checkpoint, tensorboard, earlystopping]\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=stepsPerEpoch,\n",
    "        epochs=epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=validationSteps)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model and the pre-trained weights...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resnet50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7ab3c0f8bc5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/train_small'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'resnet50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/test_small'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-b26119094884>\u001b[0m in \u001b[0;36minit_model\u001b[1;34m(train_dir, val_dir, batch_size, model_name, num_class, img_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'resnet50'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mpreprocess_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# initalize training image data generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resnet50' is not defined"
     ]
    }
   ],
   "source": [
    "model, train_generator, validation_generator = init_model(train_dir='./data/train_small', model_name='resnet50', val_dir='./data/test_small', num_class=10)\n",
    "\n",
    "train_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VffEttnvTo9r"
   },
   "source": [
    "## 3.4 Fine-tune model\n",
    "This part defines the function to fine-tune the pre-trained model on our datset. If you do not have enough data, you may consider fine-tune less layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSAFORczK7pU"
   },
   "outputs": [],
   "source": [
    " def fine_tune(model, train_generator, validation_generator, num_class=196, model_name='resnet50', batch_size=32, epochs=50, suffix='laioffer'):\n",
    "    \"\"\"\n",
    "    fine tune the model\n",
    "    parms:\n",
    "        model: initialized model\n",
    "        train_generator: training data generator\n",
    "        validation_generator: validation data generator\n",
    "        args: parsed command line arguments\n",
    "    return:\n",
    "    \"\"\"\n",
    "    # for specific architectures, define number of trainable layers\n",
    "    if model_name == 'vgg19':\n",
    "        trainable_layers = 6\n",
    "\n",
    "    for layer in model.layers[:-1*trainable_layers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in model.layers[-1*trainable_layers:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    finetune_model_name = 'finetuned_{}_{}_{}_{}.h5'.format(model_name, num_class, epochs, suffix)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}_finetune_{}\".format(model_name, time()), histogram_freq=0, write_graph=True)\n",
    "    checkpoint = ModelCheckpoint(finetune_model_name, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    earlystopping = EarlyStopping(monitor='acc', patience=5)\n",
    "    callbacks_list = [checkpoint, tensorboard, earlystopping]\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),metrics=[\"accuracy\"])\n",
    "\n",
    "    stepsPerEpoch = train_generator.samples / batch_size\n",
    "    validationSteps= validation_generator.samples / batch_size\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=stepsPerEpoch,\n",
    "        epochs=epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=validationSteps)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZY8jM59qUxsk"
   },
   "source": [
    "# 4.Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlAM5CBAURLo"
   },
   "source": [
    "## 4.1.Train/fine-tune model\n",
    "We can start to train and fine-tune our model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3080
    },
    "colab_type": "code",
    "id": "6Yvky52W4-ty",
    "outputId": "21105769-552b-4057-c3a8-f8c64fd53e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model and the pre-trained weights...\n",
      "Found 406 images belonging to 10 classes.\n",
      "Found 403 images belonging to 10 classes.\n",
      "Epoch 1/15\n",
      "13/12 [==============================] - 10s 807ms/step - loss: 6.6799 - acc: 0.1850 - val_loss: 3.4145 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.29280, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 2/15\n",
      "13/12 [==============================] - 6s 426ms/step - loss: 3.3880 - acc: 0.3594 - val_loss: 2.1889 - val_acc: 0.3772\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.29280 to 0.37717, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 3/15\n",
      "13/12 [==============================] - 6s 428ms/step - loss: 2.0571 - acc: 0.5165 - val_loss: 1.7242 - val_acc: 0.4739\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.37717 to 0.47395, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 4/15\n",
      "13/12 [==============================] - 6s 435ms/step - loss: 1.3051 - acc: 0.5857 - val_loss: 1.5906 - val_acc: 0.4988\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.47395 to 0.49876, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 5/15\n",
      "13/12 [==============================] - 6s 427ms/step - loss: 1.0635 - acc: 0.6513 - val_loss: 1.4775 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.49876 to 0.50124, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 6/15\n",
      "13/12 [==============================] - 6s 427ms/step - loss: 0.7959 - acc: 0.7714 - val_loss: 1.4767 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.50124 to 0.53102, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 7/15\n",
      "13/12 [==============================] - 6s 429ms/step - loss: 0.6887 - acc: 0.7823 - val_loss: 1.4215 - val_acc: 0.5335\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.53102 to 0.53350, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 8/15\n",
      "13/12 [==============================] - 6s 428ms/step - loss: 0.5366 - acc: 0.8202 - val_loss: 1.3835 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.53350 to 0.57320, saving model to pretrained_vgg19_10_15_laioffer.h5\n",
      "Epoch 9/15\n",
      "13/12 [==============================] - 6s 426ms/step - loss: 0.4288 - acc: 0.8703 - val_loss: 1.3753 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/15\n",
      "13/12 [==============================] - 6s 426ms/step - loss: 0.3740 - acc: 0.8892 - val_loss: 1.3946 - val_acc: 0.5533\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/15\n",
      "13/12 [==============================] - 6s 426ms/step - loss: 0.3333 - acc: 0.9003 - val_loss: 1.4078 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/15\n",
      "13/12 [==============================] - 6s 426ms/step - loss: 0.3033 - acc: 0.8992 - val_loss: 1.4693 - val_acc: 0.5608\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/15\n",
      "13/12 [==============================] - 6s 428ms/step - loss: 0.2698 - acc: 0.9113 - val_loss: 1.3852 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/15\n",
      "13/12 [==============================] - 6s 428ms/step - loss: 0.2839 - acc: 0.9082 - val_loss: 1.3486 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/15\n",
      "13/12 [==============================] - 6s 427ms/step - loss: 0.2311 - acc: 0.9388 - val_loss: 1.3588 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 1/30\n",
      "13/12 [==============================] - 7s 507ms/step - loss: 0.1898 - acc: 0.9498 - val_loss: 1.3373 - val_acc: 0.5533\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55335, saving model to finetuned_vgg19_10_30_laioffer.h5\n",
      "Epoch 2/30\n",
      "13/12 [==============================] - 6s 433ms/step - loss: 0.1573 - acc: 0.9567 - val_loss: 1.3165 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55335 to 0.56328, saving model to finetuned_vgg19_10_30_laioffer.h5\n",
      "Epoch 3/30\n",
      "13/12 [==============================] - 6s 435ms/step - loss: 0.1574 - acc: 0.9532 - val_loss: 1.2949 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.56328 to 0.57320, saving model to finetuned_vgg19_10_30_laioffer.h5\n",
      "Epoch 4/30\n",
      "13/12 [==============================] - 6s 434ms/step - loss: 0.1624 - acc: 0.9532 - val_loss: 1.3097 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/30\n",
      "13/12 [==============================] - 6s 435ms/step - loss: 0.1370 - acc: 0.9711 - val_loss: 1.2961 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.57320 to 0.58065, saving model to finetuned_vgg19_10_30_laioffer.h5\n",
      "Epoch 6/30\n",
      " 3/12 [======>.......................] - ETA: 2s - loss: 0.1392 - acc: 0.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-597c4c3645c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vgg19'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# fine-tune model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vgg19'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-f07548595699>\u001b[0m in \u001b[0;36mfine_tune\u001b[1;34m(model, train_generator, validation_generator, num_class, model_name, batch_size, epochs, suffix)\u001b[0m\n\u001b[0;32m     35\u001b[0m        \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m        \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m        validation_steps=validationSteps)\n\u001b[0m\u001b[0;32m     38\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2244\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model, train_generator, validation_generator = init_model(train_dir='./data/train_small', model_name='vgg19', val_dir='./data/test_small', num_class=10)\n",
    "# pretrain model\n",
    "train(model, train_generator, validation_generator, num_class=10,model_name='vgg19', epochs=15)\n",
    "# fine-tune model\n",
    "history = fine_tune(model, train_generator, validation_generator, num_class=10, model_name='vgg19', epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XalF3W1Nsl2v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0uHDz3RU4hL"
   },
   "source": [
    "## 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVgX3x9OU_Cx"
   },
   "source": [
    "### 4.2.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "EUB8kGALi6BA",
    "outputId": "258038a4-0905-4db1-f531-252ff8580b2d"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(validation_generator,steps=len(validation_generator))\n",
    "print(\"Validation accuracy = \", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxZag8jYVCs-"
   },
   "source": [
    "### 4.2.2 Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "2z7cWMXkXCxX",
    "outputId": "5add7cdc-c5a1-4a67-abfa-d934eb9b1aa1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# predicted_label_probs = model.predict_generator(validation_generator, verbose=1)\n",
    "# predicted_labels = np.argmax(predicted_label_probs, axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "test_datagen = image.ImageDataGenerator(\n",
    "    preprocessing_function=vgg19.preprocess_input # preprocess_input\n",
    "    )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"./data/test_small\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    shuffle=False,    # keep data in same order as labels\n",
    "    class_mode='categorical')# only data, no labels\n",
    "\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "predicted_label_probs = model.predict_generator(test_generator, verbose=1,steps=len(test_generator))\n",
    "predicted_labels = np.argmax(predicted_label_probs, axis=1)\n",
    "# predicted_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H09JCU4mmtaQ"
   },
   "outputs": [],
   "source": [
    "label_map = (test_generator.class_indices)\n",
    "# label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3427
    },
    "colab_type": "code",
    "id": "hN4lazy-XO8N",
    "outputId": "5b3be717-8578-4a60-91e7-ba75b6c104e2"
   },
   "outputs": [],
   "source": [
    "# print(confusion_matrix(true_labels, predicted_labels))\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "# print(\"Accuracy = \", accuracy_score(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhtfZiYjVK3B"
   },
   "source": [
    "### 4.2.3 Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhGxyv1r-mab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "colab_type": "code",
    "id": "Ar4Srlj2-ndE",
    "outputId": "bf0f1548-6844-484d-e361-dd084f90f4b4"
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "# Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=list(label_map.keys()),\n",
    "#                       title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix[:10,:10], classes=list(label_map.keys())[:10], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAp1JtpS-7cT"
   },
   "source": [
    "### 4.2.4 Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57aWJ0xkCofu"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "colab_type": "code",
    "id": "4Ut9-hyVqL8S",
    "outputId": "ac54ed95-adbb-44a3-b39e-e2cfa881a6e2"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW71-SVwCLKI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJUTP4WzFExR"
   },
   "outputs": [],
   "source": [
    "# predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAaA7xwXVT_S"
   },
   "source": [
    "### 4.2.5 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsdd0tOfZX7X"
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZ1V3wTWVXz1"
   },
   "source": [
    "### 4.2.6 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv2bP9CPflz1"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ItR6f9WFft7t",
    "outputId": "60d35f6e-bd22-4a3c-f56a-741253bb8bf3"
   },
   "outputs": [],
   "source": [
    "new_predicted_label_probs = new_model.predict_generator(test_generator, verbose=1,steps=len(test_generator))\n",
    "new_predicted_labels = np.argmax(new_predicted_label_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3562
    },
    "colab_type": "code",
    "id": "9XsK2BZCf_77",
    "outputId": "d37306bf-f846-4b2c-de6f-b2463922f80c"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "print(\"Accuracy = \", accuracy_score(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qF0N7XSqqBhG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "proj2_car_classification_new.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
